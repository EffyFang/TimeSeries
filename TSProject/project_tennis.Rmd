---
title: "Project_Tennis"
author: "Effy Fang"
date: "August 25, 2019"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
```

```{r package, warning=FALSE, message=FALSE}
library(fpp)
library(TSA)
library(ggplot2)
library(vars)
```

## data preparation
```{r q1}
datapath <- "C:/Users/effyf/Documents/MSCA/time series/project/TimeSeries-Project/TSProject"
project.dat<-readRDS(file=paste(datapath,"new_train.rda",sep="/"))
#head(project.dat)
tennis.en=as.integer(project.dat[,6])
tennis.fr=as.integer(project.dat[,7])
tennis.es=as.integer(project.dat[,8])
tennis.de=as.integer(project.dat[,9])
tennis=cbind(en=tennis.en,es=tennis.es,fr=tennis.fr,de=tennis.de)


# convert to ts
ts.tennis=ts(tennis,frequency = 1) #,start=c(2015,7)) 
train <- window(ts.tennis, start=1, end=775)
test <-window(ts.tennis,start=776) # test on the last 4 weeks

ts.weekly <- ts(tennis, frequency = 7)
train.weekly <- window(ts.weekly, end=c(111,5))
test.weekly <- window(ts.weekly, start=c(111,6))
tsdisplay(train)
```


the variance is not stable, need boxcox transformation 

```{r boxcox}
lambda1 <- BoxCox.lambda(train[,1])
transformed.en <- BoxCox(train[,1],lambda=lambda1)
lambda2 <- BoxCox.lambda(train[,2])
transformed.fr <- BoxCox(train[,2],lambda=lambda2)
lambda3 <- BoxCox.lambda(train[,3])
transformed.es <- BoxCox(train[,3],lambda=lambda3)
lambda4 <- BoxCox.lambda(train[,4])
transformed.de <- BoxCox(train[,4],lambda=lambda4)
transformed=cbind(en=transformed.en,es=transformed.es,fr=transformed.fr,
                de=transformed.de)
tsdisplay(transformed)
```
## stationary test 
```{r}
kpss.test(transformed.en) #is level stationary  
adf.test(transformed.en)
adf.test(transformed.es)
adf.test(transformed.fr)
adf.test(transformed.de)
```
# model fitting 

## exponential smoothing

```{r exp}
#m.ses <- ses(transformed.en)
#summary(m.ses)
#m.holt <- holt(transformed.en)
#summary(m.holt)
m.ets <- ets(transformed.en) # best amoung the 3
summary(m.ets) 
checkresiduals(m.ets)
```

residuals do not look good. we'll not consider this model

```{r}
f.ets=forecast(m.ets,h=28)
error.ets=InvBoxCox(f.ets$mean,lambda=lambda1)-test[,1]
unique(InvBoxCox(f.ets$mean,lambda=lambda1)) #only gives 1 value in the preidction 
```

## auto.arima
```{r auto.arima}
# fit auto.arima on en
m.auto=auto.arima(train[,1],lambda=lambda1)
summary(m.auto)
```

```{r}
checkresiduals(m.auto)
```
residuals look okay, pass ljung-box test if we lower the significance level. 
```{r}
f.auto <- forecast(m.auto,h=28)
error.auto <- f.auto$mean-test[,1]
#plot(forecast(m.auto,h=28),include=100)
```

## arima 
```{r}
eacf(transformed.en)
```

```{r}
m.1.0 <- Arima(train[,1], order=c(1,0,0), lambda=lambda1)
m.1.1 <- Arima(train[,1], order=c(1,0,1),lambda=lambda1)
summary(m.1.0) # better 
summary(m.1.1)
```

```{r}
checkresiduals(m.1.0)
checkresiduals(m.1.1)
```
AIC from arima(1,0,0) is not much different than auto.arima, but this model is simpler. However the redisual p-value is far from passing the ljung-box test, so we'll not consider this model. 

```{r}
f.arima.100 <- forecast(m.1.0,h=28)
error.arima.100 <- f.arima.100$mean-test[,1]
```
## sarima
```{r seasonal arima}
# reconstruct ts with 365, assume annual seasonality 
### not good! 
#annual.en=ts(transformed.en,frequency=365)
# fit auto.arima on en
#m2=auto.arima(annual.en,seasonal=T, D=1)
#summary(m2)

# reconstruct ts with 30, assume monthly seasonality 
### weekly is better.
#monthly.en=ts(transformed.en,frequency=30)
# fit auto.arima on en
#m.monthly=auto.arima(monthly.en,seasonal=T, D=1)
#summary(m.monthly)
#checkresiduals(m.monthly)
```

## weekly 
```{r weekly}
# fit auto.arima on weekly ts 
m.weekly=auto.arima(train.weekly[,1],seasonal=T,lambda=lambda1)
summary(m.weekly)
```
AIC is lower and loglikelyhood is better. 

```{r}
checkresiduals(m.weekly)
```
the residuals also look like white noise and pass ljung-box test. 

```{r}
f.sarima <- forecast(m.weekly,h=28)
error.sarima <- f.sarima$mean-test.weekly[,1]
```

## regression with arima error
adding the influence from es, fr, and de pages. 
```{r xreg}
xreg =cbind(transformed.es, transformed.fr, transformed.de)
m.reg <- auto.arima(train.weekly[,1],xreg =xreg,seasonal=T, lambda=lambda1)
summary(m.reg)
```

```{r}
checkresiduals(m.reg)
```
further lowers AIC, residuals look good. 

```{r}
transformed.test=cbind(transformed.es=BoxCox(test[,2],lambda=lambda2),
                       transformed.fr=BoxCox(test[,3],lambda=lambda3),
                       transformed.de=BoxCox(test[,4],lambda=lambda4))
f.xreg <- forecast(m.reg,xreg=transformed.test,h=28)
f.xreg
error.xreg <- f.xreg$mean-test.weekly[,1]
```

## VAR
```{r var}
VARselect(transformed) 
# 4 selected by aic, 2 by bic
m.var <- VAR(transformed,p=2,type = "both")
summary(m.var)
```

```{r}
residuals=residuals(m.var)
par(mfrow=c(2,2))
Acf(residuals[,1])
Acf(residuals[,2])
Acf(residuals[,3])
Acf(residuals[,4])
```


```{r}
par(mfrow=c(2,2))
hist(residuals[,1])
hist(residuals[,2])
hist(residuals[,3])
hist(residuals[,4])
```

```{r}
Box.test(residuals[,1],type="Ljung-Box",lag=12)
Box.test(residuals[,2],type="Ljung-Box",lag=12)
Box.test(residuals[,3],type="Ljung-Box",lag=12)
Box.test(residuals[,4],type="Ljung-Box",lag=12)
```
all the residuals look okay. 

```{r}
f.var=forecast(m.var,h=28)
f.var.en=f.var$forecast$en$mean
error.var=InvBoxCox(f.var.en, lambda=lambda1)-test[,1]
#error.var = sqrt(mean(InvBoxCox(forecast(m.var, h=28)$forecast$transformed.en$mean, lambda=lambda1)-test[,1])^2)
```

## fourier transformation - dynamic harmonic regression 
```{r fourier}
transformed.weekly.en=ts(transformed.en,frequency = 7)
harmonic <- fourier(transformed.weekly.en,K=3)
m.fourier <- auto.arima(transformed.weekly.en,xreg=harmonic, seasonal=F)
summary(m.fourier)
```

```{r}
checkresiduals(m.fourier)
```
residuals do not pass the Ljung-box test. couple of outliers in the residuals. 

```{r}
new.harmonic <- fourier(transformed.weekly.en,K=3,h=28)
f.fourier <- forecast(m.fourier,xreg=new.harmonic, h=28)
error.fourier <- InvBoxCox(f.fourier$mean, lambda=lambda1)-as.numeric(test[,1])
```

## TBATS
```{r tbats}
m.tbats <- tbats(transformed.en)
print(m.tbats)
plot(forecast(m.tbats, h=28))
checkresiduals(m.tbats)
```
residuals do not pass ljung-box test. 

##nnetar
```{r}
nn <-nnetar(transformed.en)
checkresiduals(nn)
f.nn=forecast(nn,h=28)
error.nn=InvBoxCox(f.nn$mean, lambda=lambda1)-(test[,1])
rmse.nn<-sqrt(mean(error.nn^2,na.rm=TRUE))
plot(forecast(nn, h=28))
```

## model comparison 
```{r}
aicc <- cbind(ets=m.ets$aicc, auto.arima=m.auto$aicc, arima.100=m.1.0$aicc, 
          sarima=m.weekly$aicc, xreg=m.reg$aicc, varma.AIC =AIC(m.var), fourier = m.fourier$aicc) 
aicc
sort(aicc)
bic  <- cbind(ets=m.ets$bic, auto.arima=m.auto$bic, arima.100=m.1.0$bic, 
          sarima=m.weekly$bic, xreg=m.reg$bic, varma =BIC(m.var), fourier = m.fourier$bic) 
bic
ll <-  cbind(ets=m.ets$loglik, auto.arima=m.auto$loglik, arima.100=m.1.0$loglik, 
          sarima=m.weekly$loglik, xreg=m.reg$loglik, varma =logLik(m.var), fourier = m.fourier$loglik) 
ll
rmse.ets <- sqrt(mean(error.ets^2,na.rm=TRUE))
rmse.auto <- sqrt(mean(error.auto^2,na.rm=TRUE))
rmse.arima.100 <- sqrt(mean(error.arima.100^2,na.rm=TRUE))
rmse.weekly <- sqrt(mean(error.sarima^2,na.rm=TRUE))
rmse.xreg <- sqrt(mean(error.xreg^2,na.rm=TRUE))
rmse.varma <- sqrt(mean(error.var^2,na.rm=TRUE))
rmse.fourier <- sqrt(mean(error.fourier^2,na.rm=TRUE))

rmse <- c(ets=rmse.ets, auto.arima = rmse.auto, arima.100=rmse.arima.100,
         sarima=rmse.weekly, xreg=rmse.xreg, varma=rmse.varma, fourier=rmse.fourier, nnetar=rmse.nn)
rmse
data <- rbind(aicc,bic,ll,rmse)
comparison <- as.data.frame(t(data),row.name=c('ets','auto.arima','arima(1,0,0)','sarima(weekly)', 'regression with arima error', 'varma', 'fourier'))
colnames(comparison)<-c('aic', 'bic', 'loglik', 'rmse')
comparison[order(comparison$rmse),]
```
var(2) model has a much lower aic and bic then other models, and way bigger loglikelihood. However, this is computed from all 4 equations. could not compare it with other 1 equation models. But we can compare the RMSE on the reserved test set. regression with arima error model seems to be the best fit. 

```{r}
autoplot(test.weekly[,1])+autolayer(f.xreg$mean)
```
error is bigger for the later time. it captures the general trend but underestimates the peak. 

## forecasting 1 week out
```{r prediction}
## using arima to predict es, fr and de
m.es <- auto.arima(tennis[,2],lambda='auto')
summary(m.es)
checkresiduals(m.es)
m.fr <- auto.arima(tennis[,3],lambda='auto')
summary(m.fr)
checkresiduals(m.fr)
m.de <- auto.arima(tennis[,4],lambda = 'auto')
summary(m.de)
checkresiduals(m.de)
# use predicted values to construct a new xreg to predict 1 more week of en
new.es <- forecast(m.es,h=7)$mean
new.fr <- forecast(m.fr,h=7)$mean
new.de <- forecast(m.de,h=7)$mean
# append the next 7 days prediction to the test period
new.xreg=cbind(transformed.es=BoxCox(append(test[,2],new.es),lambda=lambda2),
               transformed.fr=BoxCox(append(test[,3],new.fr),lambda=lambda3),
               transformed.de=BoxCox(append(test[,4],new.de),lambda=lambda4))
autoplot(forecast(m.reg,h=28+7, xreg=new.xreg),inlcude=50)
```