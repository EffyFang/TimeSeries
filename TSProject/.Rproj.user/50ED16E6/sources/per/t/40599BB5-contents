---
title: "hw5"
author: "Kai Li"
date: "July 29, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW5

```{r, message= FALSE}
# load libraries
library(caret)
library(TSA)
library(pls)
library(forecast)
library(tseries)

```



### Question 1:
Load the condmilk.rda dataset and split it into a training dataset (1971/1 - 1979/12) and a test dataset (1980/1 - 1980/12)
```{r, message=FALSE}

setwd("C:/Kai/UChicago/time_series/hw/")
load(file  = "condmilk.rda")


train = window(condmilk, start = c(1971,1), end = c(1979,12))
test = window(condmilk, start = c(1980,1), end = c(1980,12))
```

### Question 2:
Plot the training dataset. Is Box-Cox transformation necessary for this data?
```{r}
ts.plot(train)

kpss.test(train, null="Trend")
kpss.test(train, null="Level")


# get the lambda and tranform data by using Box-Cox method
lambda = BoxCox.lambda(train)
t_train = BoxCox(train, lambda = lambda)
```
From this time series data, the variance of later year looks smaller than the early years. However, when I use the KPSS test, the p-value 0.1 > 0.05, it is stationary. I think it might be no data for the following years. Therefore, I still consider using Box-Cox transformation. 


### Question 3:
Is the training dataset stationary? If not, find an appropriate differencing which yields seasonal and trend stationary training dataset. Plot the ACF and PACF to determine if the detrended and deseasonalized time series is stationary.
```{r}
Acf(t_train)
Pacf(t_train)

# 1st order differencing
Acf(diff(t_train))
Pacf(diff(t_train))

# 12nd order differencing
Acf(diff(t_train, lag = 12, differences = 1))
Pacf(diff(t_train, lag = 12, differences = 1))

```


```{r}
kpss.test(t_train)


kpss.test(diff(t_train, lag = 12, differences = 1))



```
From the previous plot, there is strong seasonality and the frequency is like 12. Therefore, I used lag = 12. For the PACF plot, it shows noncorrelation with t=0, but there are spikes out the limit at t =6, 11 and 12.           
I tried higher order differencing to make the data more stationary. When 7th order differencing applied, the ACF seems stationary. However, it might result in a over-differencing problem.                    
I also use the KPSS test.                          
For the KPSS test, when taking both no differencing and 1st order differencing, the p-value = 0.1 > 0.05, so it is stationary.          



### Question 4:     
Build twom ARIMA(p,d,q)(P,D,Q)s odels using the training dataset and auto.arima() function.            
Model 1: Let the auto.arima() function determine the best order of non-seasonal and seasonal differencing.              
Model 2: Set the order of seasonal-differencing d to 1 and D to 1.            
Report the resulting p,d,q,P,D,Q,s and the coefficients values for all cases and compare their AICc and BIC values.             
```{r}
# method 1
model1 = auto.arima(train, seasonal = TRUE,stepwise = TRUE,lambda = lambda)
model1


# method 2
model2 = auto.arima(train, seasonal = TRUE, d = 1, D = 1, stepwise = TRUE, lambda = lambda)
model2
```
Model 1:       
ARIMA(1,0,0)(2,1,0)[12] is the model fitted to this data.       
AICc = -410.37 and BIC = -400.55        

Model 2:          
ARIMA(1,1,1)(2,1,0)[12] is the model fitted with d = 1 and D = 1 assigned.        
AICc = -399.58 and BIC = -387.48         

Model 2 has both lower AICc and BIC, but it just a little bit lower. I would consider these two models are pretty close performance.              




### Question 5:
Plot the residuals ACF of both models from part 4 and use the Ljung-Box Test with lag 12 to verify your conclusion.
```{r}
Acf(model1$residuals)
Acf(model2$residuals)

Box.test(model1$residuals, lag = 12, type = c("Ljung-Box"))
Box.test(model2$residuals, lag = 12, type = c("Ljung-Box"))
```
For both model have residuals within the limit except a spike on t=5. I would say it is white noise. For the Box-Ljung test, it gives p-value greater than 0.05 for both models. It indicates both models are no correlation within residuals of given lags. It matches the expectation from ACF plot. Therefore, the residuals are white noise for both models.          



### Question 6:
Use both models from part 4 and the h-period argument in the forecast() function to forecast each month of 1980 (i.e., Jan, Feb, â€¦, Dec.) Plot the test dataset and forecasted values.
```{r}
h = 12

pred_1 = forecast(model1, h)

pred_2 = forecast(model2, h)



autoplot(train) +
  autolayer(pred_1, series="SARIMA")+
  autolayer(test,series = "SARIMA", color = "Blue") +
  ggtitle("Condmilk") +
  xlab("Year") + ylab("Stocks")



autoplot(train) +
  autolayer(pred_2, series="SARIMA")+
  autolayer(test,series = "SARIMA", color = "Blue") +
  ggtitle("Condmilk") +
  xlab("Year") + ylab("Stocks")


```
The blue line is the actual test data and the red line and shadows are predictions. For both two models, the prediction of t=1980 Jan is much lower than the actual stocks and at Jun the predictions also higher than the actual data. the reason for this might be after 1978 the magnitude is getting smaller. However, the training data didn't contain such data points. Therefore, the prediction has lower and higher value during the top and bottom points.          
Moreover, these two models are not significantly differencing to each other. They have pretty close performance.         



### Question 7:
Compare the forecast with the actual test data by calculating the Mean Absolute Percentage Error (MAPE) and Mean Squared Error (MSE). Which model is better to forecast the Manufacturer's Stocks for each month of 1980 (i.e., Jan, Feb, --, Dec)?
```{r}
accuracy(pred_1, test)
accuracy(pred_2, test)


MSE1 = (as.data.frame(accuracy(pred_1, test))$RMSE[2])^2
MSE2 = (as.data.frame(accuracy(pred_2, test))$RMSE[2])^2
MSE1
MSE2
MAPE1 = as.data.frame(accuracy(pred_1, test))$MAPE[2]
MAPE2 = as.data.frame(accuracy(pred_2, test))$MAPE[2]
MAPE1
MAPE2

```


MSE:         
Model1: 303.4648       
Model2: 303.4596       


MAPE:        
Model1: 18.4749          
Model2: 18.5109           

For both two models are pretty close in both MAPE and MSE. If we choose MSE as the main measurement, I would choose model 2. Otherwise, I will choose model 1. 



